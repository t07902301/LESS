{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pool -> top k instances -> fine-tune LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yiwei/miniconda3/envs/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "credit: https://github.com/prateekjoshi565/Fine-Tuning-BERT\n",
    "'''\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer, BertTokenizerFast\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "# specify GPU\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "from typing import List, Union\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_type = 'classification'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import BERT Model and BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yiwei/miniconda3/envs/venv/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/yiwei/miniconda3/envs/venv/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased', return_dict=False)\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# gte = AutoModel.from_pretrained('thenlper/gte-small', return_dict=False)\n",
    "# tokenizer = AutoTokenizer.from_pretrained('thenlper/gte-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedder = bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression(nn.Module):\n",
    "\n",
    "    def __init__(self, text_embedder):\n",
    "      \n",
    "      super(Regression, self).__init__()\n",
    "\n",
    "      self.text_embedder = text_embedder \n",
    "      \n",
    "      # dropout layer\n",
    "      self.dropout = nn.Dropout(0.1)\n",
    "      \n",
    "      # relu activation function\n",
    "      self.relu =  nn.ReLU()\n",
    "\n",
    "      # dense layer 1\n",
    "      self.fc1 = nn.Linear(768,512)\n",
    "      \n",
    "      # dense layer 2 (Output layer)\n",
    "      self.fc2 = nn.Linear(512,1)\n",
    "\n",
    "      # self.fc1 = nn.Linear(384, 3) # for GTE-small\n",
    "\n",
    "      # #softmax activation function\n",
    "      # self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "      #pass the inputs to the model  \n",
    "      _, cls_hs = self.text_embedder(sent_id, attention_mask=mask, return_dict=False)\n",
    "      # print(cls_hs.shape)\n",
    "      \n",
    "      x = self.fc1(cls_hs) # GTE-small\n",
    "\n",
    "      x = self.relu(x)\n",
    "\n",
    "      x = self.dropout(x)\n",
    "\n",
    "      # output layer\n",
    "      x = self.fc2(x)\n",
    "      \n",
    "      # # apply softmax activation\n",
    "      # x = self.softmax(x)\n",
    "\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLF(nn.Module):\n",
    "\n",
    "    def __init__(self, text_embedder):\n",
    "      \n",
    "      super(CLF, self).__init__()\n",
    "\n",
    "      self.text_embedder = text_embedder \n",
    "      \n",
    "      # dropout layer\n",
    "      self.dropout = nn.Dropout(0.1)\n",
    "      \n",
    "      # relu activation function\n",
    "      self.relu =  nn.ReLU()\n",
    "\n",
    "      # dense layer 1\n",
    "      self.fc1 = nn.Linear(768,512)\n",
    "      \n",
    "      # dense layer 2 (Output layer)\n",
    "      self.fc2 = nn.Linear(512,3)\n",
    "\n",
    "      # self.fc1 = nn.Linear(384, 3)\n",
    "\n",
    "      #softmax activation function\n",
    "      self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "      #pass the inputs to the model  \n",
    "      _, cls_hs = self.text_embedder(sent_id, attention_mask=mask, return_dict=False)\n",
    "      # print(cls_hs.shape)\n",
    "      \n",
    "      x = self.fc1(cls_hs)\n",
    "\n",
    "      x = self.relu(x)\n",
    "\n",
    "      x = self.dropout(x)\n",
    "\n",
    "      # output layer\n",
    "      x = self.fc2(x)\n",
    "      \n",
    "      # apply softmax activation\n",
    "      x = self.softmax(x)\n",
    "\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_arch(text_embedder, model_type: str):\n",
    "    if model_type == 'regression':\n",
    "        return Regression(text_embedder)\n",
    "    elif model_type == 'classification':\n",
    "        return CLF(text_embedder)\n",
    "    else:\n",
    "        raise ValueError('model_type should be either regression or classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pass the pre-trained BERT to our define architecture\n",
    "model = get_model_arch(text_embedder, model_type = filter_type)\n",
    "\n",
    "# push the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if filter_type == 'regression':\n",
    "    model.load_state_dict(torch.load('saved_regression_weights.pt'))\n",
    "else:\n",
    "    model.load_state_dict(torch.load('saved_clf_weights.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=3, bias=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Selection Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_raw_dataset(train_files: Union[List[str], str]):\n",
    "    \"\"\" load raw dataset \"\"\"\n",
    "    if isinstance(train_files, str):\n",
    "        train_files = [train_files]\n",
    "    processed_datasets = load_dataset(\n",
    "        \"json\",\n",
    "        data_files=train_files,\n",
    "    )\n",
    "    return processed_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_selection = load_raw_dataset(os.path.expanduser(\"~/data/llm/train/processed/dolly/train_dolly_data.jsonl\"))['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unfold_QA_short(data):\n",
    "    # Create a new dataset with the unfolded QA format\n",
    "    new_dataset = []\n",
    "    for QA_entry in data:\n",
    "        unfolded_QA_entry = ''\n",
    "        for QA_pair in QA_entry:\n",
    "            unfolded_QA_entry += QA_pair['role'] + ': ' + QA_pair['content'] + ' '\n",
    "        new_dataset.append(unfolded_QA_entry)\n",
    "    return new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = data_selection.map(lambda x: tokenizer(unfold_QA_short(x['messages']), padding='max_length', truncation=True, max_length=512), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool.set_format(type='torch', columns=['input_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1235090/3083731319.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pool_seq = torch.tensor(pool['input_ids'])\n",
      "/tmp/ipykernel_1235090/3083731319.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pool_mask = torch.tensor(pool['attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "# for test set\n",
    "pool_seq = torch.tensor(pool['input_ids'])\n",
    "pool_mask = torch.tensor(pool['attention_mask'])\n",
    "# pool['input_ids'].clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7505, 512])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool_seq.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed  0\n",
      "Processed  800\n",
      "Processed  1600\n",
      "Processed  2400\n",
      "Processed  3200\n",
      "Processed  4000\n",
      "Processed  4800\n",
      "Processed  5600\n",
      "Processed  6400\n",
      "Processed  7200\n"
     ]
    }
   ],
   "source": [
    "# get predictions for test data\n",
    "with torch.no_grad():\n",
    "  batch_size = 32\n",
    "  preds = []\n",
    "\n",
    "  for i in range(0, len(pool_seq), batch_size):\n",
    "    batch_seq = pool_seq[i:i+batch_size].to(device)\n",
    "    batch_mask = pool_mask[i:i+batch_size].to(device)\n",
    "    batch_preds = model(batch_seq, batch_mask)\n",
    "    preds.append(batch_preds)\n",
    "    # .detach().cpu().numpy()\n",
    "    if i % 100 == 0:\n",
    "      print(\"Processed \", i)\n",
    "      \n",
    "  preds = torch.cat(preds, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'classification'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7505, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if filter_type == 'regression':\n",
    "    preds = preds.squeeze()\n",
    "    torch.save(preds, \"../selected_data/filtered/mmlu/dolly_influence_score.pt\")\n",
    "else:\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "    preds = np.argmax(preds, axis = 1)\n",
    "    target_indices = np.arange(len(preds))[preds == 2]\n",
    "    results = data_selection.select(target_indices)\n",
    "    with open(os.path.expanduser('~/data/llm/train/processed/dolly/filtered_train_dolly_data.jsonl'), 'w', encoding='utf-8') as file:\n",
    "        for entry in results:\n",
    "            file.write(json.dumps(entry) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7505,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
