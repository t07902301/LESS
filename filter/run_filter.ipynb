{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pool -> top k instances -> fine-tune LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yiwei/miniconda3/envs/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "credit: https://github.com/prateekjoshi565/Fine-Tuning-BERT\n",
    "'''\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "# specify GPU\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "from typing import List, Union\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import BERT Model and BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yiwei/miniconda3/envs/venv/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/yiwei/miniconda3/envs/venv/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# # import BERT-base pretrained model\n",
    "# bert = AutoModel.from_pretrained('bert-base-uncased', return_dict=False)\n",
    "\n",
    "# # Load the BERT tokenizer\n",
    "# tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "gte = AutoModel.from_pretrained('thenlper/gte-small', return_dict=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained('thenlper/gte-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedder = gte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, text_embedder):\n",
    "      \n",
    "      super(Model_Arch, self).__init__()\n",
    "\n",
    "      self.text_embedder = text_embedder \n",
    "      \n",
    "      # dropout layer\n",
    "      self.dropout = nn.Dropout(0.1)\n",
    "      \n",
    "      # relu activation function\n",
    "      self.relu =  nn.ReLU()\n",
    "\n",
    "      # # dense layer 1\n",
    "      # self.fc1 = nn.Linear(768,512)\n",
    "      \n",
    "      # # dense layer 2 (Output layer)\n",
    "      # self.fc2 = nn.Linear(512,3)\n",
    "\n",
    "      self.fc1 = nn.Linear(384, 3)\n",
    "\n",
    "      #softmax activation function\n",
    "      self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "      #pass the inputs to the model  \n",
    "      _, cls_hs = self.text_embedder(sent_id, attention_mask=mask, return_dict=False)\n",
    "      # print(cls_hs.shape)\n",
    "      \n",
    "      x = self.fc1(cls_hs)\n",
    "\n",
    "      # x = self.relu(x)\n",
    "\n",
    "      # x = self.dropout(x)\n",
    "\n",
    "      # # output layer\n",
    "      # x = self.fc2(x)\n",
    "      \n",
    "      # apply softmax activation\n",
    "      x = self.softmax(x)\n",
    "\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pass the pre-trained BERT to our define architecture\n",
    "model = Model_Arch(text_embedder)\n",
    "\n",
    "# push the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('saved_gte_weights.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Selection Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_raw_dataset(train_files: Union[List[str], str]):\n",
    "    \"\"\" load raw dataset \"\"\"\n",
    "    if isinstance(train_files, str):\n",
    "        train_files = [train_files]\n",
    "    processed_datasets = load_dataset(\n",
    "        \"json\",\n",
    "        data_files=train_files,\n",
    "    )\n",
    "    return processed_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import disable_caching\n",
    "# disable_caching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_selection = load_raw_dataset(\"../data/train/processed/dolly/train_dolly_data.jsonl\")['train']\n",
    "# data_selection = load_dataset('json', data_files='../data/train/processed/dolly/train_dolly_data.jsonl', download_mode='force_redownload')['train']\n",
    "# import json\n",
    "# data_selection = []\n",
    "# with open(\"../data/train/processed/dolly/train_dolly_data.jsonl\", 'r', encoding='utf-8', errors='ignore') as file:\n",
    "#     for line in file:\n",
    "#         data_selection.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# # wrap tensors\n",
    "# val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# # # sampler for sampling the data during training\n",
    "# # val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# # dataLoader for validation set\n",
    "# val_dataloader = DataLoader(val_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_selection = data_selection.map(cache_file_name=data_selection.cache_files[0]['filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_selection.cleanup_cache_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'role': 'user',\n",
       "   'content': \"Task: When did Virgin Australia start operating?\\n\\nVirgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\\n\\nAnswer:\"},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.'}],\n",
       " [{'role': 'user', 'content': 'Which is a species of fish? Tope or Rope'},\n",
       "  {'role': 'assistant', 'content': '\\nTope'}]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pool['messages'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered = []\n",
    "# with open(\"../data/train/processed/dolly/filtered_train_dolly_data.jsonl\", 'r', encoding='utf-8', errors='ignore') as file:\n",
    "#     for line in file:\n",
    "#         filtered.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def unfold_QA_short(data):\n",
    "#     # Create a new dataset with the unfolded QA format\n",
    "#     new_dataset = []\n",
    "#     for text_entry in data['messages']:\n",
    "#         unfolded_text_entry = ''\n",
    "#         for text_pair in text_entry:\n",
    "#             unfolded_text_entry += text_pair['role'] + ': ' + text_pair['content'] + ' '\n",
    "#         new_dataset.append(unfolded_text_entry)\n",
    "#     return new_dataset\n",
    "\n",
    "def unfold_QA_short(data):\n",
    "    # Create a new dataset with the unfolded QA format\n",
    "    new_dataset = []\n",
    "    for QA_entry in data:\n",
    "        unfolded_QA_entry = ''\n",
    "        for QA_pair in QA_entry:\n",
    "            unfolded_QA_entry += QA_pair['role'] + ': ' + QA_pair['content'] + ' '\n",
    "        new_dataset.append(unfolded_QA_entry)\n",
    "    return new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7505/7505 [00:01<00:00, 4358.06 examples/s]\n"
     ]
    }
   ],
   "source": [
    "pool = data_selection.map(lambda x: tokenizer(unfold_QA_short(x['messages']), padding='max_length', truncation=True, max_length=512), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool.set_format(type='torch', columns=['input_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1528050/2599539357.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pool_seq = torch.tensor(pool['input_ids'])\n",
      "/tmp/ipykernel_1528050/2599539357.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pool_mask = torch.tensor(pool['attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "# for test set\n",
    "pool_seq = torch.tensor(pool['input_ids'])\n",
    "pool_mask = torch.tensor(pool['attention_mask'])\n",
    "# pool['input_ids'].clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7505, 512])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yiwei/miniconda3/envs/venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# max_seq_len = 512\n",
    "# # tokenize and encode sequences in the test set\n",
    "# tokens_pool = tokenizer.batch_encode_plus(\n",
    "#     unfold_QA_short(pool['messages']),\n",
    "#     max_length = max_seq_len,\n",
    "#     pad_to_max_length=True,\n",
    "#     truncation=True,\n",
    "#     return_token_type_ids=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test set\n",
    "pool_seq = torch.tensor(tokens_pool['input_ids'])\n",
    "pool_mask = torch.tensor(tokens_pool['attention_mask'])\n",
    "# test_y = torch.tensor(get_labels(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed  0\n",
      "Processed  800\n",
      "Processed  1600\n",
      "Processed  2400\n",
      "Processed  3200\n",
      "Processed  4000\n",
      "Processed  4800\n",
      "Processed  5600\n",
      "Processed  6400\n",
      "Processed  7200\n"
     ]
    }
   ],
   "source": [
    "# get predictions for test data\n",
    "with torch.no_grad():\n",
    "  batch_size = 32\n",
    "  preds = []\n",
    "\n",
    "  for i in range(0, len(pool_seq), batch_size):\n",
    "    batch_seq = pool_seq[i:i+batch_size].to(device)\n",
    "    batch_mask = pool_mask[i:i+batch_size].to(device)\n",
    "    batch_preds = model(batch_seq, batch_mask)\n",
    "    preds.append(batch_preds.detach().cpu().numpy())\n",
    "    if i % 100 == 0:\n",
    "      print(\"Processed \", i)\n",
    "    # print(\"Processed \", i)\n",
    "\n",
    "  preds = np.concatenate(preds, axis=0)\n",
    "  # preds = model(pool_seq.to(device), pool_mask.to(device))\n",
    "  # preds = preds.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model's performance\n",
    "preds = np.argmax(preds, axis = 1)\n",
    "# print(classification_report(test_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7505,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 3, 5, 6])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_indices = np.arange(len(preds))[preds == 2]\n",
    "target_indices[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = data_selection[target_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subset of the dataset by selecting specific indices\n",
    "# subset_indices = [0, 1, 2, 3, 4]  # Example indices\n",
    "results = data_selection.select(target_indices)\n",
    "\n",
    "# # Display the subset\n",
    "# print(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dataset', 'id', 'messages'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# no need if all train_data.jsonl is used\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m keys \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m()\n\u001b[1;32m      3\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m target_indices:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "# no need if all train_data.jsonl is used\n",
    "keys = results.keys()\n",
    "results = []\n",
    "for idx in target_indices:\n",
    "    results.append({key:pool[key][idx]  for key in keys})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5482"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save the data rows of text into a JSONL file\n",
    "with open('../data/train/processed/dolly/filtered_train_dolly_data.jsonl', 'w', encoding='utf-8') as file:\n",
    "    for entry in results:\n",
    "        file.write(json.dumps(entry) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[:\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'train'"
     ]
    }
   ],
   "source": [
    "results['train'][:2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
